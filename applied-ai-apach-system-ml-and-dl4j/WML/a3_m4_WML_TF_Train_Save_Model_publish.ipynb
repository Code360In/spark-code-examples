{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<body>\n",
    "    <table style=\"border: none\" align=\"center\">\n",
    "        <tr style=\"border: none\">\n",
    "            <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"45\" width=\"45\"></th>\n",
    "            <th style=\"border: none\"><font face=\"verdana\" size=\"6\" color=\"black\"><b>Watson Machine Learning</b></font></th>\n",
    "        </tr>\n",
    "    </table>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WML - Tensorflow Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contens:\n",
    "1. Train a Tensorflow model in ** DSX Notebook **\n",
    "2. Save the trained model into WML Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Versions:\n",
    "1. Python Runtime: Python 3.5\n",
    "2. Tensorflow version: 1.2 \n",
    "3. Anaconda Runtime Version: Anaconda 4.2.9 for Python 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory Requirements for Online Deployment and Scoring:\n",
    "1. The model to be deployed using Online Deployment and Scoring service should be persisted in WML Repository \n",
    "2. The persisted model should contain Tensorflow signature metadata for serving(Refer section 2.1). Online deployment is restricted for those persisted Tensorflow models that do not contain this meta data. This requirement will be adequately documented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"center\">\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/scikit-learn/hand-written-digits-recognition/images/numbers_banner-04.png\" width=\"600\" alt=\"Icon\"> </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the use case:- Recognition of hand written digits\n",
    "Using Tensorflow, we train a model that can recognize a handwritten number embedded in an image. The model is trained using the MNIST data set that can be accessed using the Tensorflow's sample dataset related APIs. Here, we use Tensorflow's implementation of Convolutional Neural Network(CNN) to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Train a Tensorflow model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below, let us download the datasets that we will use for training, validation and test purposes. The APIs used here provides us the datasets in form of a **single dimensional NumPy array** which has been transformed from the actual images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set parameters required for creating the input and the target(label) Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_input - Refers to the size of the single dimensional array that represents one image containing hand written number <br>\n",
    "n_classes - Refers to the number of possible categories of prediction outcomes. In this use case, the prediction outcome can be any of the **10 digits** i.e 0 -9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define the input and the target(label) Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x - A placeholder that will hold the input data for training and scoring <br>\n",
    "y - A placeholder that will hold the numeric value of the hand written number in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input], name=\"x_input\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Set the convolutional neural network related parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional neural network that we are going to build requires weights and biases to be initialized for each of the convolution layer in the network. The code below initializes these weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize each convolution layer's weights, bias and dropout\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    \"wc1\": tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    \"wc2\": tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    \"wd1\": tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    \"out\": tf.Variable(tf.random_normal([1024, n_classes])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"bc1\": tf.Variable(tf.random_normal([32])),\n",
    "    \"bc2\": tf.Variable(tf.random_normal([64])),\n",
    "    \"bd1\": tf.Variable(tf.random_normal([1024])),\n",
    "    \"out\": tf.Variable(tf.random_normal([n_classes])),\n",
    "}\n",
    "\n",
    "dropout = 0.75\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Build the model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tensorflow, a model is built by building a computational graph. The computational graph is in turn built by defining the nodes. Each node refers to a placeholder or a transformation operation of the input data. The source of input data for a node could either be another node in the graph or from the user specified at time executing the graph. In Tensorflow's terms, each node is called as a Tensor. <br>\n",
    "\n",
    "In the cell below, we define the Tensors(nodes) in the graph that implement convolutional neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input picture\n",
    "x_trans1 = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "# Convolution Layer -1\n",
    "x_conv2d_l1 = tf.nn.conv2d(x_trans1, weights[\"wc1\"], strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "x_w_bias_l1 = tf.nn.bias_add(x_conv2d_l1, biases[\"bc1\"])\n",
    "x_relu_l1 = tf.nn.relu(x_w_bias_l1)\n",
    "conv1_out = tf.nn.max_pool(\n",
    "    x_relu_l1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convolution Layer -2\n",
    "x_conv2d_l2 = tf.nn.conv2d(\n",
    "    conv1_out, weights[\"wc2\"], strides=[1, 1, 1, 1], padding=\"SAME\"\n",
    ")\n",
    "x_w_bias_l2 = tf.nn.bias_add(x_conv2d_l2, biases[\"bc2\"])\n",
    "x_relu_l2 = tf.nn.relu(x_w_bias_l2)\n",
    "conv2_out = tf.nn.max_pool(\n",
    "    x_relu_l2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\"\n",
    ")\n",
    "\n",
    "# Fully connected layer\n",
    "# Reshape conv2 output to fit fully connected layer input\n",
    "fc1 = tf.reshape(conv2_out, [-1, weights[\"wd1\"].get_shape().as_list()[0]])\n",
    "fc1 = tf.add(tf.matmul(fc1, weights[\"wd1\"]), biases[\"bd1\"])\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "# Apply Dropout\n",
    "fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "# Output, class prediction\n",
    "conv_out = tf.add(tf.matmul(fc1, weights[\"out\"]), biases[\"out\"], name=\"output_tensor\")\n",
    "\n",
    "predictor = tf.argmax(conv_out, 1, name=\"predictor\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=conv_out, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# To Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(conv_out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Set parameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_iters - Refers to the number of images that we plan to use for training the model. Using more images for training leads to better accuracy of the model <br>\n",
    "batch_size - The training will be performed iteratively on a batch of images. batch_size refers to the number of images that needs to be part of the batch <br>\n",
    "display_step  - Refers to the n-th iteration of training after which the training accuracy data will be calculated and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "training_iters = 60000\n",
    "batch_size = 128\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Initialize a Tensorflow Session to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model refers to executing the computational graph that holds the model defintion. <br>\n",
    "Tensorflow uses a C++ backend application to execute the computational graph. The connection to the C++ backend application from Tensorflow's Python runtime is managed by Session object. We hence initialize a Session using the code below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "# Keep training until reach max iterations\n",
    "while step * batch_size < training_iters:\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "    print(\"Completed batch iteration: \" + str(step * batch_size))\n",
    "    if step % display_step == 0:\n",
    "        # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        print(\n",
    "            \"Iter \"\n",
    "            + str(step * batch_size)\n",
    "            + \", Minibatch Loss= \"\n",
    "            + \"{:.6f}\".format(loss)\n",
    "            + \", Training Accuracy= \"\n",
    "            + \"{:.5f}\".format(acc)\n",
    "        )\n",
    "    step += 1\n",
    "print(\"Model training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We have now trained a Tensorflow model. As a next step we need to persist this model in WML Repository **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Save the trained Tensorflow model in WML Repository using Repository's Python Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create the signature of the tensors that will be required for scoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signature refers to the information about the Tensors that hold the input data and the output data for scoring. This signature will be used at the time of scoring using the WML Online Deployment and Scoring service. <br>\n",
    "\n",
    "As per our model definition, <br> \n",
    "the Tensor - \"x\" is the placeholder that holds the input data of the model and <br>\n",
    "the Tensor - \"predictor\" is the node that holds the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P.S : \n",
    "** This is a mandatory requirement for scoring the model. Hence this data should be provided while saving the model. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_inputs = tf.saved_model.utils.build_tensor_info(x)\n",
    "classification_outputs_classes = tf.saved_model.utils.build_tensor_info(predictor)\n",
    "\n",
    "classification_signature = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "    inputs={tf.saved_model.signature_constants.CLASSIFY_INPUTS: classification_inputs},\n",
    "    outputs={\n",
    "        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES: classification_outputs_classes\n",
    "    },\n",
    "    method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME,\n",
    ")\n",
    "\n",
    "print(\"classification_signature content:\")\n",
    "print(classification_signature)\n",
    "\n",
    "legacy_op_init = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Save the model using WML Repository's Python client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must import client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from repository_v3.mlrepository import MetaNames, MetaProps\n",
    "from repository_v3.mlrepositoryartifact import MLRepositoryArtifact\n",
    "from repository_v3.mlrepositoryclient import MLRepositoryClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Provide WML instance credentials and authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Watson Machine Learning service on Bluemix.\n",
    "\n",
    "**Action:** Add authentication information from your instance of Watson Machine Learning service here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WML Instance details\n",
    "\n",
    "# SVT\n",
    "# service_url = \"https://ibm-watson-ml-svt.stage1.mybluemix.net\"\n",
    "service_url = \"https://ibm-watson-ml.mybluemix.net\"\n",
    "user = \"10c5f8f1-ab84-4501-91e1-dda53fc438cd\"\n",
    "password = \"24ec7cc5-250f-4bdf-8a94-1200be3d1cb7\"\n",
    "instance_id = \"d8b98cb2-cd06-4740-8467-def50eca91f9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_repository_client = MLRepositoryClient(service_url)\n",
    "ml_repository_client.authorize(user, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Create a WML Artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the metadata about the model that wish to persist along with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_name = \"k_tf_mnist_10203\"\n",
    "tf_model_metadata = {\n",
    "    MetaNames.DESCRIPTION: \"Tensorflow model for predecting Hand-written digits\",\n",
    "    MetaNames.AUTHOR_EMAIL: \"krishna@in.ibm.com\",\n",
    "    MetaNames.AUTHOR_NAME: \"Krishna\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a WML Repository artifact by specifying the session object that contains the graph of the model that we wish to save in WML Repository and the scoring signature of the model. We also specify other metadata that we want to save along the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_artifact = MLRepositoryArtifact(\n",
    "    sess,\n",
    "    signature_def_map={\"predict_images\": classification_signature},\n",
    "    legacy_init_op=legacy_op_init,\n",
    "    name=tf_model_name,\n",
    "    meta_props=MetaProps(tf_model_metadata.copy()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Save the model to WML Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below serializes the model artifact that contains reference to the Session object and the related details and saves it in WML Repository as a compressed tar ball. <br>\n",
    "The API returns a bunch of metadata that was created as part of saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(tf_model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display few metadata of our interest that was generated as part of saving the model to the WML Repository. <br>\n",
    "\n",
    "modelVersionUrl displayed in output of the cell below refers to the WML Repository URL that points to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid = saved_model.uid\n",
    "model_ver_url = saved_model.meta.prop(\"modelVersionUrl\")\n",
    "print(\n",
    "    \"ModelType: \"\n",
    "    + saved_model.meta.prop(\"frameworkName\")\n",
    "    + \"-\"\n",
    "    + saved_model.meta.prop(\"frameworkVersion\")\n",
    ")\n",
    "print(\"ModelId: \" + saved_model.uid)\n",
    "print(\"modelVersionUrl: \" + saved_model.meta.prop(\"modelVersionUrl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we now have persisted a trained model in WML Repository, we are ready to deploy and score using this model. The deployment and scoring functionality is explained in the notebook named \"WML_TF_Serving_Using_Onllike_Deploy_Scoring_Service\" in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header 'Authorization: Bearer eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbnRJZCI6ImQ4Yjk4Y2IyLWNkMDYtNDc0MC04NDY3LWRlZjUwZWNhOTFmOSIsImluc3RhbmNlSWQiOiJkOGI5OGNiMi1jZDA2LTQ3NDAtODQ2Ny1kZWY1MGVjYTkxZjkiLCJwbGFuSWQiOiIzZjZhY2Y0My1lZGU4LTQxM2EtYWM2OS1mOGFmM2JiMGNiZmUiLCJyZWdpb24iOiJ1cy1zb3V0aCIsInVzZXJJZCI6IjEwYzVmOGYxLWFiODQtNDUwMS05MWUxLWRkYTUzZmM0MzhjZCIsImlzcyI6Imh0dHBzOi8vaWJtLXdhdHNvbi1tbC5teWJsdWVtaXgubmV0L3YzL2lkZW50aXR5IiwiaWF0IjoxNTE2OTYyNzkzLCJleHAiOjE1MTY5OTE1OTMsImFwcGxpY2F0aW9uSWQiOiI5NjQ2YTA0NC03YmUyLTQyYTYtYmIxMy0zNjUyYzI0NDYyMTQifQ.WtB_Vx7KQW1AFMfdbZV4xEA2NgOJun2OoeLOLCaUNPMmlrdGmZw00E6kT96F8KvwhtfTPOUzBx46bqg1Y1nZ30W2XtjsenNNQoDZnfJxySvzc2pC0ixW3KhfFujG857frJj1cbu3WPhd0PQMeojuEbD2GOod_N339rX-mcZHhctkw0cS-sgTUilggz0R8gS9C2bmk_gWl05xYW1M4Ayb-4bbQnvt6rumdJjTxk2Sf_iH0plDmVNblUNA5Jdb0IYjZYg7sbbZW-SXJzqwolc-6Nelne5fvlouUuujuFkboxJpNeCoyNusexOnnRaM2DANZwEv7RlwnxniEGgrxOC8Wg' -d '{\"inputs\":[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19607844948768616, 0.8784314393997192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27450981736183167, 0.11372549831867218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4745098352432251, 0.9058824181556702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5803921818733215, 0.658823549747467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01568627543747425, 0.7647059559822083, 0.9058824181556702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3764706254005432, 0.8235294818878174, 0.04313725605607033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2705882489681244, 0.988235354423523, 0.5254902243614197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44705885648727417, 0.988235354423523, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1764705926179886, 0.9254902601242065, 0.8509804606437683, 0.0470588281750679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7529412508010864, 0.988235354423523, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.658823549747467, 0.9686275124549866, 0.20784315466880798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07058823853731155, 1.0, 0.9921569228172302, 0.08235294371843338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3294117748737335, 0.9490196704864502, 0.8274510502815247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5529412031173706, 0.9921569228172302, 0.7411764860153198, 0.019607843831181526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.41568630933761597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125490203499794, 0.9098039865493774, 0.9803922176361084, 0.25882354378700256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05882353335618973, 0.8823530077934265, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5254902243614197, 0.988235354423523, 0.8274510502815247, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08627451211214066, 0.988235354423523, 0.6431372761726379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.6549019813537598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03529411926865578, 0.8000000715255737, 0.8196079134941101, 0.07058823853731155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08627451211214066, 0.9921569228172302, 0.9921569228172302, 0.41960787773132324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6627451181411743, 0.988235354423523, 0.7803922295570374, 0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.3333333432674408, 0.5058823823928833, 0.6431372761726379, 0.7647059559822083, 0.988235354423523, 0.988235354423523, 0.41568630933761597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16078431904315948, 0.6666666865348816, 0.960784375667572, 0.988235354423523, 0.988235354423523, 0.988235354423523, 0.988235354423523, 0.9098039865493774, 0.9058824181556702, 0.9843137860298157, 0.988235354423523, 0.988235354423523, 0.03529411926865578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19215688109397888, 0.3294117748737335, 0.3294117748737335, 0.3294117748737335, 0.3294117748737335, 0.0, 0.0, 0.6313725709915161, 0.988235354423523, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49803924560546875, 0.988235354423523, 0.988235354423523, 0.1764705926179886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.501960813999176, 0.9921569228172302, 0.9921569228172302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49803924560546875, 0.988235354423523, 0.988235354423523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.529411792755127, 0.988235354423523, 0.9568628072738647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9098039865493774, 0.9254902601242065, 0.43529415130615234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7019608020782471, 0.25882354378700256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]}' 'https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac/online'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x, y = mnist.train.next_batch(batch_size)\n",
    "json.dumps(np.array(x[0]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X GET --header 'Accept: application/json' --header 'Authorization: Bearer eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbnRJZCI6ImQ4Yjk4Y2IyLWNkMDYtNDc0MC04NDY3LWRlZjUwZWNhOTFmOSIsImluc3RhbmNlSWQiOiJkOGI5OGNiMi1jZDA2LTQ3NDAtODQ2Ny1kZWY1MGVjYTkxZjkiLCJwbGFuSWQiOiIzZjZhY2Y0My1lZGU4LTQxM2EtYWM2OS1mOGFmM2JiMGNiZmUiLCJyZWdpb24iOiJ1cy1zb3V0aCIsInVzZXJJZCI6IjEwYzVmOGYxLWFiODQtNDUwMS05MWUxLWRkYTUzZmM0MzhjZCIsImlzcyI6Imh0dHBzOi8vaWJtLXdhdHNvbi1tbC5teWJsdWVtaXgubmV0L3YzL2lkZW50aXR5IiwiaWF0IjoxNTE2OTYyNzkzLCJleHAiOjE1MTY5OTE1OTMsImFwcGxpY2F0aW9uSWQiOiI5NjQ2YTA0NC03YmUyLTQyYTYtYmIxMy0zNjUyYzI0NDYyMTQifQ.WtB_Vx7KQW1AFMfdbZV4xEA2NgOJun2OoeLOLCaUNPMmlrdGmZw00E6kT96F8KvwhtfTPOUzBx46bqg1Y1nZ30W2XtjsenNNQoDZnfJxySvzc2pC0ixW3KhfFujG857frJj1cbu3WPhd0PQMeojuEbD2GOod_N339rX-mcZHhctkw0cS-sgTUilggz0R8gS9C2bmk_gWl05xYW1M4Ayb-4bbQnvt6rumdJjTxk2Sf_iH0plDmVNblUNA5Jdb0IYjZYg7sbbZW-SXJzqwolc-6Nelne5fvlouUuujuFkboxJpNeCoyNusexOnnRaM2DANZwEv7RlwnxniEGgrxOC8Wg' 'https://ibm-watson-ml.mybluemix.net/v3/wml_instances/d8b98cb2-cd06-4740-8467-def50eca91f9/published_models/3a982151-54b5-462f-9006-2243587c6af1/deployments/1aa6e50c-ce52-495b-9ac5-2adad19d07ac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = mnist.test.images[\n",
    "    45,\n",
    "].tolist()\n",
    "image2 = mnist.test.images[\n",
    "    4,\n",
    "].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = {\"inputs\": [image1, image2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
