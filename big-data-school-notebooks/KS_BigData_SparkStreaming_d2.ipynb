{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "965b7416-6a75-469e-81bf-a20330163e14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import unbase64\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "# Source with default settings\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([50.0, 20.0, 20.0])),\n",
    "    (0.0, Vectors.dense([60.0, 30.0, 30.0])),\n",
    "    (0.0, Vectors.dense([70, 25.0, 250.0])),\n",
    "    (1.0, Vectors.dense([65, 18.0, 18.0]))], [\"label\", \"features\"])\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model1.extractParamMap())\n",
    "\n",
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n",
    "\n",
    "# You can combine paramMaps, which are python dictionaries.\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "\n",
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "print(model2.extractParamMap())\n",
    "\n",
    "\n",
    "\n",
    "#connectionString = \"Endpoint=sb://iothub-ns-ks-bigdata-5809426-7048d14abf.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=sQQPHOQhXdvNxTBcBjTfz2fWOTTTnRwRZu+/9HKbXmM=;EntityPath=ks-bigdata-iot-hub-test\"\n",
    "connectionString = \"Endpoint=sb://iothub-ns-ks-bigdata-5809426-7048d14abf.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=sQQPHOQhXdvNxTBcBjTfz2fWOTTTnRwRZu+/9HKbXmM=;EntityPath=ks-bigdata-iot-hub-test\"\n",
    "\n",
    "\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString' : connectionString\n",
    "}\n",
    "\n",
    "ehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "\n",
    "# Schema of incoming data from IoT hub\n",
    "schema = \"timestamp timestamp, temperature double, humidity double, deviceId string, status string\"\n",
    "\n",
    "# Read directly from IoT Hub using the EventHubs library for Databricks\n",
    "iot_stream = (\n",
    "  spark.readStream.format(\"eventhubs\")                                              # Read from IoT Hubs directly\n",
    "    .options(**ehConf)                                                              # Use the Event-Hub-enabled connect \n",
    "    .load()                                                                         # Load the data\n",
    "    .withColumn('reading', F.from_json(F.col('body').cast('string'), schema))       # Extract the \"body\" payload from  \n",
    ")\n",
    "\n",
    "#display(iot_stream)\n",
    "\n",
    "iot_stream = iot_stream.withColumn('temperature', iot_stream.reading.temperature).withColumn('humidity', iot_stream.reading.humidity).withColumn('deviceId', iot_stream.reading.deviceId).withColumn('ts', iot_stream.reading.timestamp)\n",
    "iot_stream2 = iot_stream.groupBy(window(iot_stream.ts, \"5 seconds\", \"3 seconds\"), iot_stream.deviceId).agg(avg('humidity'), avg('temperature'), min('humidity'), min('temperature'), max('humidity'), max('temperature'))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"avg(humidity)\", 'avg(temperature)', 'avg(temperature)'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "features = assembler.transform(iot_stream2.withColumn('tsp', iot_stream2.window.start))\n",
    "#print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "#output.select(\"features\", \"clicked\").show(truncate=False)\n",
    "\n",
    "predictions = model1.transform(features)\n",
    "predictions.printSchema\n",
    "display(predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "KS_BigData_SparkStreaming_d2",
   "notebookOrigID": 3094614850090714,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
