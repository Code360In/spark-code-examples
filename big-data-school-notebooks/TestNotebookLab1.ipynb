{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb29cb40-12e3-48ab-980e-8a55c02f2a97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating DataFrame from RDD with with predefined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bdb33573-a3fd-4f54-82c9-86ccd9dc20b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# any RDD (of known structure :) \n",
    "r = sc.parallelize([('John',1940),('Paul',1942),('George',1943),('Ringo',1940)])\n",
    "\n",
    "# converting RDD elements to the special \"Row\" object\n",
    "r_row = r.map(lambda x: Row(name=x[0], year=x[1]))\n",
    "\n",
    "# creating DataFrame from RDD of rows\n",
    "r_df = spark.createDataFrame(r_row)\n",
    "\n",
    "# and registering it as temporary view to enable sql queries\n",
    "r_df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "# Note: \"spark\" is a predefined SparkSession object\n",
    "ages = spark.sql(\"SELECT df.name, df.year, year(current_date()) - df.year as age FROM df ORDER by df.year\")\n",
    "\n",
    "ages.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5b5de53f-46b0-4e9d-a4ff-63273601257d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a8490f7-d0cd-45e2-85ed-999ed6334c00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# any RDD\n",
    "r = sc.parallelize([('John',1940),('Paul',1942),('George',1943),('Ringo',1940)])\n",
    "\n",
    "# and some schema description\n",
    "schema_str = 'Name string,Year long'\n",
    "\n",
    "# build a list of fields from schema description string\n",
    "fields = []\n",
    "for field in schema_str.split(','):\n",
    "    field_name,type_name = field.split(' ')\n",
    "    # the following \"if\" could be longer to contain all needed types\n",
    "    if type_name == 'long':\n",
    "        type = LongType()\n",
    "    else:\n",
    "        type = StringType()\n",
    "    # add new field to the list\n",
    "    fields.append(StructField(field_name, type, True)) # all fields are nullable for simplicity\n",
    "    \n",
    "\n",
    "# define schema as a list of fields\n",
    "schema = StructType(fields)\n",
    "\n",
    "# combine data and schema into DataFrame\n",
    "df = spark.createDataFrame(r, schema)\n",
    "\n",
    "# just another convinient way to see DataFrame contents\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e9014b6-0f6a-49c8-a1d7-b65c616b83e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/sample_07.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField(\"code\", StringType(), True),        \n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"total_emp\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \"\\t\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .schema(customSchema) \\\n",
    "  .load(file_location)\n",
    "\n",
    "\n",
    "df.printSchema\n",
    "#df.show(n = 10, truncate = False)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04499fea-aa6b-45ac-8be4-d85dbb065e99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f35835bc-400a-4ae2-8baa-22a82a241f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# new column from expression, renaming columns, columnar projection\n",
    "df.withColumn(\"total_salary\",df.salary*df.total_emp)\\\n",
    "  .withColumn(\"new_column\",lit(0))\\\n",
    "  .withColumnRenamed(\"salary\",\"avg_salary\")\\\n",
    "  .show()\n",
    "  \n",
    "#.select(\"code\",\"total_emp\",\"avg_salary\",\"total_salary\")\\\n",
    "#.drop(\"RIP\")\\\n",
    "\n",
    "#df.withColumn(\"total_salary\",df.salary.cast(LongType())*df.total_emp)\\\n",
    "#  .withColumnRenamed(\"salary\",\"avg_salary\")\\\n",
    "#  .select(\"code\",\"total_emp\",\"avg_salary\",\"total_salary\")\\\n",
    "#  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e932f936-21c9-4123-bf00-91ff54bb6c02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DataFrame API: filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "711818dd-704b-4a95-8892-766887d1e45a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# some standard columnar filters\n",
    "# + SQL-like filtering strings\n",
    "df.where(df.salary > 50000)\\\n",
    "  .where(df.total_emp.between(100000,300000))\\\n",
    "  .where('code = \"11-1011\" or code = \"11-3011\"')\\\n",
    "  .where(df.description.like('A%'))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f23e50c1-8962-4a0b-b5f5-2cba7635827b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DataFrame API: aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "433e5968-b155-4403-98b4-47d240608fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple aggregation methods of GroupedData class: avg, count, max, min, sum\n",
    "# quite convinient, but does not allow different aggregation functions\n",
    "df.withColumn(\"code_major\",substring(df.code,1,2))\\\n",
    "  .groupBy(\"code_major\")\\\n",
    "  .avg(\"total_emp\",\"salary\")\\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7abc032f-493a-4434-9286-9cecdc61c7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "16b86706-90a8-4536-90ab-884443b164cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    # `v` is a pandas Series\n",
    "    return v.add(1)  # outputs a pandas Series\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e41d72b-e394-4b68-8a13-2496893f7590",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.createOrReplaceTempView(\"sample_07\")\n",
    "\n",
    "df = spark.sql('select * from sample_07')\n",
    "\n",
    "\n",
    "# unified \"agg\" method with dictionary argument (key = column, value = function)\n",
    "# note: expression instead of column is not supported\n",
    "# allows to mix different aggregation functions\n",
    "df.withColumn(\"code_major\",substring(df.code,1,2))\\\n",
    "  .groupBy(\"code_major\")\\\n",
    "  .agg({\"*\":\"count\",\"salary\": \"avg\", \"total_emp\": \"sum\"})\\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18f468aa-7880-4cec-9603-52e641fb73f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df = spark.sql('select * from sample_07')\n",
    "\n",
    "\n",
    "# unified \"agg\" method with columnar expressions\n",
    "# allows to mix different aggregation functions + convinient renaming via .alias\n",
    "# supports aggregation on expressions\n",
    "df.withColumn(\"code_major\",substring(df.code,1,2))\\\n",
    "  .groupBy(\"code_major\")\\\n",
    "  .agg(sf.round(sf.avg(df.salary*0.13),2).alias(\"avg_salary_tax\")\\\n",
    "      ,sf.round(sf.sum(df.total_emp),2).alias(\"sum_employees\")\\\n",
    "      )\\\n",
    "  .show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d1068a0-3748-4385-9b5d-50e758d0f9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DataFrame API: joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "24b13f6f-25c5-4c5b-b29e-3970edf80c15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df = spark.sql('select * from sample_07')\n",
    "df = df.withColumn(\"code_major\",sf.substring(df.code,1,2))\n",
    "\n",
    "# create another DataFrame for join\n",
    "level_1 = spark.sql(\"select code, description from sample_07 where code like '%-0000'\")\n",
    "level_1 = level_1.withColumn(\"code_major\",sf.substring(level_1.code,1,2))\n",
    "\n",
    "\n",
    "# Third argument is the join type\n",
    "# allowed join types are: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi (aka exists), and left_anti (aka not exists)\n",
    "\n",
    "df.join(level_1, level_1.code_major == df.code_major, 'inner')\\\n",
    "  .select(df.code.alias(\"child_code\"),level_1.code.alias(\"parent_code\"))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdda55bc-03d6-4d90-8c60-922928697bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Dataframe execution plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efcf6a9a-d688-435a-8cb7-13b710a791bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "df = spark.sql('select * from sample_07')\n",
    "df = df.withColumn(\"code_major\",sf.substring(df.code,1,2))\n",
    "\n",
    "# create another DataFrame for join\n",
    "level_1 = spark.sql(\"select code, description from sample_07 where code like '%-0000'\")\n",
    "level_1 = level_1.withColumn(\"code_major\",sf.substring(level_1.code,1,2))\n",
    "\n",
    "test = df.join(level_1, level_1.code_major == df.code_major, 'inner')\\\n",
    "  .select(df.code.alias(\"child_code\"),level_1.code.alias(\"parent_code\"))\n",
    "  \n",
    "  \n",
    "print(test.explain(extended = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01c77cc6-2d0a-4980-a920-34662bbeffd7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d # Writing to storage\n",
    " \n",
    "DBFS root\n",
    "The default storage location in DBFS is known as the DBFS root. Several types of data are stored in the following DBFS root locations:\n",
    "\n",
    "\n",
    "/FileStore: Imported data files, generated plots, and uploaded libraries.\n",
    "\n",
    "/databricks-datasets: Sample public datasets. See Special DBFS root locations.\n",
    "\n",
    "/databricks-results: Files generated by downloading the full results of a query.\n",
    "\n",
    "/databricks/init: Global and cluster-named (deprecated) init scripts.\n",
    "\n",
    "/user/hive/warehouse: Data and metadata for non-external Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8de33f9-de61-4509-b28d-455d9a33945c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's read that CSV\n",
    "#df = spark.read.csv(\"/FileStore/movies.csv\", inferSchema = True, header = True)\n",
    "\n",
    "#df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/df/Sample.csv\")\n",
    "\n",
    "# and write it back as a compressed CSV\n",
    "df.write.csv(\"/FileStore/movies.bzip2\", compression = 'bzip2', header = True)\n",
    "\n",
    "# and read it back (a good idea to check, esp. for backups ;) )\n",
    "df2 = spark.read.csv(\"/FileStore/movies.bzip2\", inferSchema = True, header = True)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "TestNotebookLab1",
   "notebookOrigID": 2449292102727165,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
