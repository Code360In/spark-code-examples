{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing with ML Pipelines\n",
    "\n",
    "Spark ML offers a nice Pipeline API for building more complex transformation and machine learning pipelines. We will use these building blocks in this exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "First we need to load data from S3 or HDFS. We read the data as CSV using Sparks builtin CSV parser, but still we need to create a schema to specify column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('row_id', IntegerType(), True),\n",
    "        StructField('date', StringType(), True),\n",
    "        StructField('season', IntegerType(), True),\n",
    "        StructField('year', IntegerType(), True),\n",
    "        StructField('month', IntegerType(), True),\n",
    "        StructField('hour', IntegerType(), True),\n",
    "        StructField('holiday', IntegerType(), True),\n",
    "        StructField('weekday', IntegerType(), True),\n",
    "        StructField('workingday', IntegerType(), True),\n",
    "        StructField('weather', IntegerType(), True),\n",
    "        StructField('temperature', DoubleType(), True),\n",
    "        StructField('apparent_temperature', DoubleType(), True),\n",
    "        StructField('humidity', DoubleType(), True),\n",
    "        StructField('wind_speed', DoubleType(), True),\n",
    "        StructField('casual', IntegerType(), True),\n",
    "        StructField('registered', IntegerType(), True),\n",
    "        StructField('counter', IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = spark.read.schema(schema).csv(\n",
    "    's3://dimajix-training/data/bike-sharing/hour_nohead.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data\n",
    "\n",
    "Let us have a look at the first 10 entries again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for ML\n",
    "\n",
    "Again we need to transform all numerical (and also categorical) entries to Doubles. This is required by most algorithms in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ddata = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n",
    "\n",
    "Before diving into the real world of Spark ML Pipelines, we split the data into a training set and a test set. Let us use 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split ddata into train_data and test_data\n",
    "# YOUR CODE HERE\n",
    "...\n",
    "\n",
    "print(\"train_data: %d\" % train_data.count())\n",
    "print(\"test_data: %d\" % test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a ML Pipeline\n",
    "\n",
    "Now we will create out first very simple pipeline using all numerical variables as features. This can be done very easily. We already know the two relevant classes performing the actual work\n",
    "\n",
    "    VectorAssembler - extracts all features and stores them inside a Vector\n",
    "    LinearRegression - performs the regression\n",
    "    \n",
    "We create a ML Pipeline with these two components. As features we'll again use the columns\n",
    "\n",
    "    year, season, month, hour, holiday, weekday, workingday,\n",
    "    weather, temperature, apparent_temperature, humidity, wind_speed\n",
    "    \n",
    "and of course we want to predict \"counter\". The prediction shall be stored in \"prediction\".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *\n",
    "\n",
    "\n",
    "# Create a Pipeline with multiple stages. You will probably need a VectorAssembler and a LinearRegression stage.\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit model using the Pipeline\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Data\n",
    "\n",
    "Now that we have a model, we want to perform predictions for the test data. And let us also print a table with the first 10 entries of the predicted DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some Pictures again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we need to import matplotlib.pyplot and add some magic to display the plots inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the total number of rents per day again, and let's compare that visually against the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily = (\n",
    "    prediction.groupBy('ts').agg({'counter': 'sum', 'prediction': 'sum'}).orderBy('ts')\n",
    ")\n",
    "\n",
    "pdf = daily.toPandas()\n",
    "\n",
    "min_ts, max_ts = prediction.agg(min('ts'), max('ts')).collect()[0]\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k', tight_layout=True)\n",
    "plt.plot(pdf['ts'], pdf['sum(counter)'])\n",
    "plt.plot(pdf['ts'], pdf['sum(prediction)'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([min_ts, max_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "\n",
    "Again we want to evaluate the resulting model using RegressionEvaluator from package pyspark.ml.evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "rmse = ...\n",
    "\n",
    "print(\"RMSE of Simple Model = %f\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logarithmic Metric\n",
    "\n",
    "In this example, we might not be so much interested about the absolute prediction error, but more about a relative prediction error. This can be expressed very well on a logarithmic scale. But we cannot use builtin evaluators for that, we need to create one on our own.\n",
    "\n",
    "But first let us try to calculate the RMSE metric manually. RMSE is defined as\n",
    "\n",
    "    sqrt(avg((predicted_value - true_value)**2)\n",
    "    \n",
    "And the the Root Mean Squared Logarithmic Error is defined as\n",
    "\n",
    "    sqrt(avg((log(predicted_value) - log(true_value))**2)\n",
    "    \n",
    "Both metrics can be easily implemented using standard functionality of Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "rmse = ...\n",
    "rmsle = ...\n",
    "\n",
    "print(\"RMSE = %f\" % rmse)\n",
    "print(\"RMSLE = %f\" % rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarthmic Model\n",
    "\n",
    "Since our error metric is now in logarithmic space, it makes sense to optimize in that space, too. Therefore we switch to a logarithmic model.\n",
    "\n",
    "We implement the logarithmic model by applying the following transformation to the \"counter\" column before fitting the linear model:\n",
    "\n",
    "    lcounter = log(counter + 1)\n",
    "    \n",
    "Then we fit a linear regression model to the target variable lcounter (instead of counter). The predicted value should be stored in a column 'lprediction'.\n",
    "\n",
    "But since eventually we are interested in the linear value (and not in the logarithmic value), we backtransform the predicted value from the logarithmic scale into the linear scale by\n",
    "\n",
    "    prediction = exp(lprediction) - 1\n",
    "    \n",
    "In order to perform the Transformation, we can add multiple SQLTransformer at appropriate locations to the Pipeline. An SQLTransformer has one keyword argument\n",
    "\n",
    "    SQLTransformer(statement=\"SELECT x+y AS z,* FROM __THIS__\")\n",
    "    \n",
    "which will create DataFrames with a new column 'z' which is the sum of both columns 'x' and 'y'. Different SQLTransformers can be used for the transformation of the counter and lprediction variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The pipeline should have (in some correct order)\n",
    "#  1x LinearRegression\n",
    "#  2x SQLTransformer\n",
    "#  1x VectorAssembler\n",
    "pipeline = Pipeline(stages=[...])\n",
    "\n",
    "# Fit model using the Pipeline\n",
    "logmodel = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Again we want to calculate the RMSE and RMSLE for the test data using the new logarithmic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "logprediction = ...\n",
    "\n",
    "rmse = ...\n",
    "rmsle = ...\n",
    "\n",
    "print(\"RMSE = %f\" % rmse)\n",
    "print(\"RMSLE = %f\" % rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some Pictures again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily = (\n",
    "    logprediction.groupBy('ts').agg({'counter': 'sum', 'prediction': 'sum'}).orderBy('ts')\n",
    ")\n",
    "\n",
    "pdf = daily.toPandas()\n",
    "\n",
    "min_ts, max_ts = logprediction.agg(min('ts'), max('ts')).collect()[0]\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k', tight_layout=True)\n",
    "plt.plot(pdf['ts'], pdf['sum(counter)'])\n",
    "plt.plot(pdf['ts'], pdf['sum(prediction)'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([min_ts, max_ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding More Features\n",
    "\n",
    "We might want to add more features in order to improve prediction quality. We propose the following additional features:\n",
    "\n",
    "1. Features for modelling period effects of a year. This can be done by adding the two features:\n",
    "        sin(ts / 31536000 * 6.28318531) \n",
    "        cos(ts / 31536000 * 6.28318531)\n",
    "2. Similarily for modelling periodic effects within a week, the following features can be used:\n",
    "        sin(weekday / 7 * 6.28318531)\n",
    "        cos(weekday / 7 * 6.28318531)\n",
    "3. And for modelling periodic effects within a single day the following features can be used:\n",
    "        sin(hour / 24 * 6.28318531)\n",
    "        cos(hour / 24 * 6.28318531)\n",
    "4. season, one-hot encoded\n",
    "5. weather, one-hot encoded\n",
    "\n",
    "You can use SQLTransformer for arithmetic transformations and a combination of\n",
    "\n",
    "    StringIndexer(inputCol='categoricalFeature', outputCol='categoricalIndex')\n",
    "    OneHotEncoder(inputCol='categoricalIndex', outputCol='categoricalOneHot')\n",
    "    \n",
    "for creating one hot encoded categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Pipeline should have\n",
    "#  1x LinearRegression\n",
    "#  2x OneHotEncoder\n",
    "#  2x StringIndexer\n",
    "#  3x SQLTransformer (or maybe more)\n",
    "#  1x VectorAssembler\n",
    "pipeline2 = Pipeline(stages=[...])\n",
    "\n",
    "logmodel2 = pipeline2.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate new Model\n",
    "\n",
    "Again we want to evaluate our new model using RMSE and RMSLE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logprediction2 = logmodel2.transform(test_data)\n",
    "\n",
    "rmse = ...\n",
    "rmsle = ...\n",
    "\n",
    "print(\"RMSE = %f\" % rmse)\n",
    "print(\"RMSLE = %f\" % rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 2.1 (Python 3.5)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
