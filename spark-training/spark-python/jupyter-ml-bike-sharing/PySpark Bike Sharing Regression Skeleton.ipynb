{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "raw_data = sc.textFile('/user/cloudera/data/bike-sharing/hour_nohead.csv')\n",
    "column_data = raw_data.map(lambda x: x.split(','))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('row_id',StringType(),True),\n",
    "    StructField('date',StringType(), True),\n",
    "    StructField('season',StringType(), True),\n",
    "    StructField('year',StringType(), True),\n",
    "    StructField('month',StringType(), True),\n",
    "    StructField('hour',StringType(), True),\n",
    "    StructField('holiday',StringType(), True),\n",
    "    StructField('weekday',StringType(), True),\n",
    "    StructField('workingday',StringType(), True),\n",
    "    StructField('weather',StringType(), True),\n",
    "    StructField('temperature',StringType(), True),\n",
    "    StructField('apparent_temperature',StringType(), True),\n",
    "    StructField('humidity',StringType(), True),\n",
    "    StructField('wind_speed',StringType(), True),\n",
    "    StructField('casual',StringType(), True),\n",
    "    StructField('registered',StringType(), True),\n",
    "    StructField('counter',StringType(), True)\n",
    "    ])\n",
    "structured_data = sqlContext.createDataFrame(column_data, schema)    \n",
    "data = structured_data.select(\n",
    "    structured_data.row_id.cast('int'),\n",
    "    structured_data.date.cast('string'),\n",
    "    structured_data.season.cast('int'),\n",
    "    structured_data.year.cast('int'),\n",
    "    structured_data.month.cast('int'),\n",
    "    structured_data.hour.cast('int'),\n",
    "    structured_data.holiday.cast('int'),\n",
    "    structured_data.weekday.cast('int'),\n",
    "    structured_data.workingday.cast('int'),\n",
    "    structured_data.weather.cast('int'),\n",
    "    structured_data.temperature.cast('double'),\n",
    "    structured_data.apparent_temperature.cast('double'),\n",
    "    structured_data.humidity.cast('double'),\n",
    "    structured_data.wind_speed.cast('double'),\n",
    "    structured_data.casual.cast('int'),\n",
    "    structured_data.registered.cast('int'),\n",
    "    structured_data.counter.cast('int')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "ddata = data.select(\n",
    "        data.date,\n",
    "        unix_timestamp(data.date, \"yyyy-MM-dd\").alias('ts'),\n",
    "        data.season.cast(\"double\"),\n",
    "        data.year.cast(\"double\"),\n",
    "        data.month.cast(\"double\"),\n",
    "        data.hour.cast(\"double\"),\n",
    "        data.holiday.cast(\"double\"),\n",
    "        data.weekday.cast(\"double\"),\n",
    "        data.workingday.cast(\"double\"),\n",
    "        data.weather.cast(\"double\"),\n",
    "        data.temperature,\n",
    "        data.apparent_temperature,\n",
    "        data.humidity,\n",
    "        data.wind_speed,\n",
    "        data.casual.cast(\"double\"),\n",
    "        data.registered.cast(\"double\"),\n",
    "        data.counter.cast(\"double\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some Pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import matplotlib.pyplot and also make all plots appear inline in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Plot of Rents per Day\n",
    "The original data contains rents per hour, we want to have the data per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Pandas DataFrame with summed data per day\n",
    "pdf = ...\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(counter)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now only look at casual renters\n",
    "pdf = ...\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(casual)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now only look at registered renters\n",
    "pdf = ...\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(pdf['ts'],pdf['sum(registered)'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Statistics\n",
    "\n",
    "Of course we are interested in some initial statistics on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = ddata.schema\n",
    "\n",
    "for field in schema.fields:\n",
    "    # Print statistcs for field if field is Double Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Vectors for Regression\n",
    "\n",
    "Spark ML needs a special data type (Vector) for most operations. So we need to transform columns of interest into that special data type.\n",
    "\n",
    "A Vector can be created from a double Array via\n",
    "\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    Vectors.dense([1.0,2.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_vector(row, cols):\n",
    "    pass\n",
    "\n",
    "print extract_vector(Row('name','age')('Bob',23), [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform DataFrame\n",
    "\n",
    "Now that we have extract_vector, we can use it in order to extract the relevant features from our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the following columns\n",
    "cols = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "# Transform all records ddata into vectors [feature, counter]\n",
    "# counter can be found in column row[16]\n",
    "rdd = ...\n",
    "\n",
    "# Now create new DataFrame\n",
    "features_labels = sqlContext.createDataFrame(rdd, ['features','counter'])\n",
    "\n",
    "# Peek inside, convert first 10 rows to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = ...\n",
    "print train_data.count()\n",
    "print test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peek into the Model\n",
    "\n",
    "Let us have a look at the coefficients and at the intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Prediction\n",
    "\n",
    "Predict new Data by applying the model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use VectorAssembler\n",
    "\n",
    "Manual feature extraction (i.e. creation of the Vector) is a little bit tedious and not very comfortable. But luckily, there is a valuable helper called VectorAssembler.\n",
    "\n",
    "We use it to automatically extract the columns\n",
    "\n",
    "    season, year, month, hour, holiday, weekday, workingday, weather, \n",
    "    temperature, apparent_temperature, humidity, wind_speed\n",
    "    \n",
    "into the new output column 'features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Data\n",
    "\n",
    "Since we found an easier way to generate features, we split incoming data first and apply the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = ddata.randomSplit([0.8,0.2], seed=0)\n",
    "print train_data.count()\n",
    "print test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Regression\n",
    "\n",
    "1. Apply VectorAssembler\n",
    "2. Perform Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asm = ...\n",
    "regression = ...\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Make predictions from test data and print some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally lets evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make New Pictures of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = prediction \\\n",
    "    .groupBy('ts').agg({'counter':'sum', 'prediction':'sum'}) \\\n",
    "    .orderBy('ts')\n",
    "    \n",
    "pdf = tmp.toPandas()\n",
    "\n",
    "min_ts,max_ts = prediction.agg(min('ts'), max('ts')).collect()[0]\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k', tight_layout=True)\n",
    "plt.plot(pdf['ts'],pdf['sum(counter)'])\n",
    "plt.plot(pdf['ts'],pdf['sum(prediction)'])    \n",
    "axes = plt.gca()\n",
    "axes.set_xlim([min_ts,max_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
