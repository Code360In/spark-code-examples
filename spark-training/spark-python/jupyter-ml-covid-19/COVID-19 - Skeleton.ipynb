{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19\n",
    "\n",
    "This notebook analyzes the growth of the COVID-19 pandemy. It relies on the data provided by Johns Hopkins CSSE at https://github.com/CSSEGISandData/COVID-19 . The main question is: how will the number of infected people change over time. We will use a very simple approach, that should not be used for serious predictions of spreads of deseases, but which is well supported in PySpark. For a better mathematical model, please read https://de.wikipedia.org/wiki/SIR-Modell . Unfortunately there is no support in PySpark for estimating model parameters within a more meaningful model. \n",
    "\n",
    "So this notebook is mainly about getting some basic insights into machine learning with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Spark Context & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"64G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "The original data is available at https://github.com/CSSEGISandData/COVID-19 provided by Johns Hopkins CSSE. There are several different representations of the data, we will peek into different versions and then select the most appropriate to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = 's3://dimajix-training/data/covid-19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Time Series\n",
    "\n",
    "The repository already contains time series data. This is nice to look at, but specifically for PySpark maybe a little bit hard to work with. Each line in the file contains a full time series of the number of positive tested persons. This means that the number of columns change with every update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = spark.read\\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(basedir + \"/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\")\n",
    "series.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Daily Reports\n",
    "\n",
    "The repository also contains more detailed files containing the daily reports of the total number of positively tested persons. Within those files, every line represents exactly one region and one time. Therefore the schema stays stable with every update, only new records are appended. But there are some small technical challenges that we need to take. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date parser helper\n",
    "\n",
    "First small challenge: All records contain a date, some of them a datetime. But the format has changed several times. In order to handle the different cases, we provide a small PySpark UDF (User Defined Function) that is capable of parsing all formats and which returns the extracted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "@f.udf(DateType())\n",
    "def parse_date(date):\n",
    "    if \"/\" in date:\n",
    "        date = date.split(\" \")[0]\n",
    "        (m,d,y) = date.split(\"/\")\n",
    "        y = int(y)\n",
    "        m = int(m)\n",
    "        d = int(d)\n",
    "        if (y < 2000):\n",
    "            y += 2000\n",
    "    else:\n",
    "        date = date[0:10]\n",
    "        (y,m,d) = date.split(\"-\")\n",
    "        y = int(y)\n",
    "        m = int(m)\n",
    "        d = int(d)\n",
    "    return datetime.date(year=y,month=m,day=d)\n",
    "\n",
    "#print(parse_date(\"2020-03-01\"))\n",
    "#print(parse_date(\"1/22/2020\"))\n",
    "#print(parse_date(\"2020-03-01T23:45:50\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data, old schema\n",
    "\n",
    "Next challenge is that the schema did change, namely between 2020-03-21 and 2020-03-22. The column names have changed, new columns have been added and so on. Therefore we cannot read in all files within a single `spark.read.csv`, but we need to split them up into two separate batches with different schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last date to read\n",
    "today = datetime.date(2020, 4, 7) #datetime.date.today()\n",
    "# First date to read\n",
    "start_date = datetime.date(2020, 1, 22)\n",
    "# First date with new schema\n",
    "schema_switch_date = datetime.date(2020, 3, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first bunch of files is stored as CSV and has the following columns:\n",
    "* `Province_State`\n",
    "* `Country_Region`\n",
    "* `Last_Update` date of the last update\n",
    "* `Confirmed` the number of confirmed cases\n",
    "* `Deaths` the number of confirmed cases, which have died\n",
    "* `Recovered` the number of recovered cases\n",
    "* `Latitude` and `Longitude` geo coordinates of the province\n",
    "\n",
    "The metrics (confirmed, deaths and recovered) are always totals, they already contain all cases from the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_reports_dir = basedir + \"/csse_covid_19_daily_reports\"\n",
    "\n",
    "# Define old schema for first batch of files\n",
    "schema_1 = StructType([\n",
    "    StructField(\"Province_State\", StringType()),\n",
    "    StructField(\"Country_Region\", StringType()),\n",
    "    StructField(\"Last_Update\", StringType()),\n",
    "    StructField(\"Confirmed\", LongType()),\n",
    "    StructField(\"Deaths\", LongType()),\n",
    "    StructField(\"Recovered\", LongType()),\n",
    "    StructField(\"Latitude\", DoubleType()),\n",
    "    StructField(\"Longitude\", DoubleType()),\n",
    "])\n",
    "\n",
    "# Generate all dates with old schema\n",
    "schema_1_dates = [start_date + datetime.timedelta(days=d) for d in range(0,(schema_switch_date - start_date).days)]\n",
    "# Generate file names with old schema\n",
    "schema_1_files = [daily_reports_dir + \"/\" + d.strftime(\"%m-%d-%Y\") + \".csv\" for d in schema_1_dates]\n",
    "\n",
    "# Read in all files with old schema\n",
    "cases_1 = spark.read\\\n",
    "    .schema(schema_1) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(schema_1_files)\n",
    "\n",
    "# Peek inside\n",
    "cases_1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data, new schema\n",
    "\n",
    "Now we perform exactly the same logical step, we read in all files with the new schema. The second bunch of files is stored as CSV and has the following columns:\n",
    "* `FIPS` country code\n",
    "* `Admin2` administrative name below province (i.e. counties)\n",
    "* `Province_State`\n",
    "* `Country_Region`\n",
    "* `Last_Update` date of the last update\n",
    "* `Latitude` and `Longitude` geo coordinates of the province\n",
    "* `Confirmed` the number of confirmed cases\n",
    "* `Deaths` the number of confirmed cases, which have died\n",
    "* `Recovered` the number of recovered cases\n",
    "* `Active` the number of currently active cases\n",
    "* `Combined_Key` a combination of `Admin2`, `Province_State` and `Country_Region`\n",
    "\n",
    "The metrics (confirmed, deaths and recovered) are always totals, they already contain all cases from the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "daily_reports_dir = basedir + \"/csse_covid_19_daily_reports\"\n",
    "\n",
    "# New schema\n",
    "schema_2 = StructType([\n",
    "    StructField(\"FIPS\", StringType()),\n",
    "    StructField(\"Admin2\", StringType()),\n",
    "    StructField(\"Province_State\", StringType()),\n",
    "    StructField(\"Country_Region\", StringType()),\n",
    "    StructField(\"Last_Update\", StringType()),\n",
    "    StructField(\"Latitude\", DoubleType()),\n",
    "    StructField(\"Longitude\", DoubleType()),\n",
    "    StructField(\"Confirmed\", LongType()),\n",
    "    StructField(\"Deaths\", LongType()),\n",
    "    StructField(\"Recovered\", LongType()),\n",
    "    StructField(\"Active\", LongType()),\n",
    "    StructField(\"Combined_Key\", StringType())\n",
    "])\n",
    "\n",
    "# Generate all dates with new schema\n",
    "schema_2_dates = [schema_switch_date + datetime.timedelta(days=d) for d in range(0,(today- schema_switch_date).days)]\n",
    "# Generate file names with new schema\n",
    "schema_2_files = [daily_reports_dir + \"/\" + d.strftime(\"%m-%d-%Y\") + \".csv\" for d in schema_2_dates]\n",
    "\n",
    "# Read in all CSV files with new schema\n",
    "cases_2 = spark.read\\\n",
    "    .schema(schema_2)  \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(schema_2_files)\n",
    "\n",
    "cases_2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unify Records\n",
    "\n",
    "Now we union both data sets `cases_1` and `cases_2` into a bigger data set with a common schema. The target schema should contain the following columns:\n",
    "* `Country_Region`\n",
    "* `Province_State`\n",
    "* `Admin2`\n",
    "* `Last_Update`\n",
    "* `Confirmed`\n",
    "* `Deaths`\n",
    "* `Recovered`\n",
    "\n",
    "In case a specific column is not present in onw of the two input DataFrames, simply provide a NULL value (`None` in Python) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases = \\\n",
    "    cases_1.select(\n",
    "        f.col(\"Country_Region\"),\n",
    "        f.col(\"Province_State\"),\n",
    "        f.lit(None).cast(StringType()).alias(\"Admin2\"),\n",
    "        f.col(\"Last_Update\"),\n",
    "        f.col(\"Confirmed\"),\n",
    "        f.col(\"Deaths\"),\n",
    "        f.col(\"Recovered\")\n",
    "    ).union(\n",
    "    cases_2.select(\n",
    "        f.col(\"Country_Region\"),\n",
    "        f.col(\"Province_State\"),\n",
    "        f.col(\"Admin2\"),\n",
    "        f.col(\"Last_Update\"),\n",
    "        f.col(\"Confirmed\"),\n",
    "        f.col(\"Deaths\"),\n",
    "        f.col(\"Recovered\")\n",
    "    )\n",
    "    )\n",
    "\n",
    "all_cases.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Aggragate\n",
    "\n",
    "The records can contain multiple updates per day. But we only want to have the latest update per administrative region on each day. Therefore we perform a simple grouped aggregation and simply pick the maximum of all metrics of interest (`Confirmed`, `Deaths`, `Recovered`). This means we require a grouped aggregation with the grouping keys `Last_Update`, `Country_Region`, `Province_State` and `Admin2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_eod = # YOUR CODE HERE\n",
    "\n",
    "all_cases_eod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Sanity Checks\n",
    "\n",
    "Since we have now a nice data set containing all records, lets peek inside and let us perform some sanity checks if the numbers are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_eod.where(f.col(\"Country_Region\") == f.lit(\"US\")) \\\n",
    "    .orderBy(f.col(\"Confirmed\").desc()) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count cases in US\n",
    "\n",
    "Let us count the cases in the US for a specific date, maybe compare it to some resource on the web. This can be done by summing up all confirmed case where `Country_Region` equals to `US` and where `Last_Update` equals some date of your choice (for example `2020-04-05`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count cases in Germany\n",
    "\n",
    "Let us now sum up the confirmed cases for Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases_eod.where(f.col(\"Country_Region\") == f.lit(\"Germany\")) \\\n",
    "    .where(f.col(\"Last_Update\") == f.lit(\"2020-04-06\")) \\\n",
    "    .select(f.sum(f.col(\"Confirmed\"))) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inspect & Visualize\n",
    "\n",
    "Now that we have a meaningful dataset, let us create some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Additional Preparations\n",
    "\n",
    "Before doing deeper analyzis, we still need to perform some simple preparations in order to make the resuls more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases pre country and day\n",
    "\n",
    "We are not interested in the specific numbers of different provinces or states within a single country. The problem with data per province is that they may contain too few cases for following any theoretical law or for forming any meaningful probability distribution. Therefore we sum up all cases per country per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_cases = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age in days\n",
    "\n",
    "Before continuing, we will add one extra column, namely the day of the epedemy for every country. The desease started on different dates in different countries (start being defined as the date of the first record in the data set). To be able to compare the development of the desease between different countries, it is advisable to add a country specific `day` column, which simply counts the days since the first infection in the particular country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "all_countries_age_cases = all_country_cases \\\n",
    "    .withColumn(\"First_Update\", f.min(f.col(\"Last_Update\")).over(Window.partitionBy(\"Country_Region\").orderBy(\"Last_Update\"))) \\\n",
    "    .withColumn(\"day\", f.datediff(f.col(\"Last_Update\"), f.col(\"First_Update\")))\n",
    "\n",
    "all_countries_age_cases.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pick single country\n",
    "\n",
    "For the whole analysis, we focus on a single country. I decided to pick Germany, but you can also pick a different country.\n",
    "\n",
    "The selection can easily be done by filtering using the column `Country_Region` to contain the desired country (for example `Germany`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_cases = all_countries_age_cases.where(f.col(\"Country_Region\") == \"Germany\")\n",
    "\n",
    "# Show first 10 days of data in the correct order\n",
    "country_cases.orderBy(f.col(\"day\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plot\n",
    "\n",
    "Let us make a simple plot which shows the number of cases over time. This can be achieved by using the matplotlib function `plt.plot`, which takes two arguments: the data on the horizontal axis (x-axis) and the data on the vertical axis (y-axis). One can also specify the size of the plot by using the function `plt.figure` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = country_cases.toPandas()\n",
    "\n",
    "# Set size of the figure\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Make an image usig plt.plot\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot on logarithmic scale\n",
    "\n",
    "The spread of a epedemy follows an exponential pattern (specifically at the beginning), this can also be seen from the plot above. Therefore it is a good idea to change the scale from a linear scale to a logarithmic scale. With the logarithmic scale, you can spot the relativ rate of increase, which is the slope of the curve. Changing the scale can easily be done with the function `plt.yscale('log')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = country_cases.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[\"day\"], df[\"Confirmed\"], color='blue', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of daily increases\n",
    "\n",
    "In this case, we are also interested in the number of new cases on every day. This means, that we need to subtract the current number of confirmed cases from the last number of confirmed cases. This is a good example where so called *windowed aggregation* can help us in PySpark. Normally all rows of a DataFrame are processed independently, but for this question (the difference of the number of confirmed cases between two days), we would need to access the rows from two different days. That can be done with window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_increase = country_cases.withColumn(\n",
    "        \"Confirmed_Increase\", \n",
    "        f.col(\"Confirmed\") - f.last(f.col(\"Confirmed\")).over(Window.partitionBy(\"Country_Region\").orderBy(\"day\").rowsBetween(-100,-1))\n",
    "    )\n",
    "\n",
    "daily_increase.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an additional column \"Confirmed_Increase\", which we can now plot. A continuous line plot doesn't make so much sense, since the metric is very discrete by its nature. Therefore we opt for a bar chart instead by using the method `plt.bar` instead of `plt.plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daily_increase.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Make  a bar plot using plt.bar\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning\n",
    "\n",
    "Now we want to use some methods of machine learning in order to predict the further development of the desease within a single country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Split Data\n",
    "\n",
    "The very first step in every machine learning project is to split up the whole data into two data sets, the *training data* and the *validation data*. The basic idea is that in order to validate our model, we need some data that was not used during training. That is what the *validation data* will be used for. If we did not exclude the data from the training, we could only infer information about how good the model fits to our data - but we would have no information about how good the model copes with new data. And the second aspect is crucial in prediction applications, where the model will be used on new data points which have never been seen before.\n",
    "\n",
    "There are different approaches how to do that, but not all approaches work in every scenario. In our use case we are looking at a time series, therefore we need to split the data at a specific date - we need to hide out some information for the training phase. For time series it is important not to perform a random sampling, since this would imply information creep from the future. I.e. if we exclude day 40 and include day 50 in our training data set, day 50 obviously has some information on day 40, which normally would not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_days = country_cases.select(\"day\").orderBy(f.col(\"day\")).distinct().collect()\n",
    "all_days = [row[0] for row in all_days]\n",
    "num_days = len(all_days)\n",
    "cut_day = all_days[int(0.7*num_days)]\n",
    "print(\"cut_day = \" + str(cut_day))\n",
    "\n",
    "# We might want to skip some days where there was no real growth\n",
    "first_day = 28\n",
    "\n",
    "# Select training records from first_day until cut_day (inclusive)\n",
    "training_records = # YOUR CODE HERE\n",
    "\n",
    "# Select validation records from cut_day (exclusive)\n",
    "validation_records = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Simple Regression\n",
    "\n",
    "The most simple approach is to use a very basic linear regression. We skip this super simple approach, since we already know that our data has some exponential ingredients. Therefore we already use a so called *generalized linear model* (GLM), which transforms our values into a logarithmic space before performing a linear regression. Here we already know that this won't work out nicely, since the plots above already indicate a curved shape over time - something a trivial linear model cannot catch. We will take care of that in a later step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark ML Pipelines\n",
    "\n",
    "Spark ML encourages to use so called *pipelines*, which combine a list of transformation blocks. For the very first very simple example, we need two building blocks:\n",
    "* `VectorAssembler` is required to collect all features into a single column of the special type `Vector`. Most machine learning algorithms require the independant variables (the predictor variables) to packaged together into a single column of type `Vector`. This can be easily done by using the `VectorAssembler`-\n",
    "* `GeneralizedLinearRegression` provides the regression algorithm a a building block. It needs to be configured with the indepedant variable (the features column), the dependant variable (the label column) and the prediction column where the predictions should be stored in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    # YOUR CODE HERE\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model\n",
    "\n",
    "Once we have specified all building blocks in the pipeline, we can *fit* the whole pipeline to obtain a *pipeline model*. The fitting operation either applies a transformation (like the `VectorAssembler`) or recursively fits any embedded estimator (like the `GeneralizedLinearRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform prediction\n",
    "\n",
    "One we have obtained the model, it can be used as a transformer again in order to produce predictions. For plotting a graph, we will apply the model not only to the validation set, but to the whole data set. This can be done with the `model.transform` method applied to the `country_cases` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "Finally we want to visualize the real values and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[\"day\"], df[\"Confirmed\"], color='blue', lw=2)\n",
    "plt.plot(df[\"day\"], df[\"Predict\"], color='red', lw=2)\n",
    "plt.vlines(cut_day, ymin=0, ymax=400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Perfrmance\n",
    "\n",
    "The picture already doesn't make much hope that the model generalizes well to new data. But in order to compare different models, we should also quantify the fit. We use a built in metric called *root mean squared error* provided by the class `RegressionEvaluator`. One an instance of the class is created, you can evaluate the predictions by using the `evaluate` function.\n",
    "\n",
    "Since we are only interested in the ability to generalize, we use the `validation_records` DataFrame for measuring the qulity of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = # YOUR CODE HERE\n",
    "\n",
    "pred = model.transform(validation_records)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Improve Model\n",
    "\n",
    "The first shot is not very satisfactory, specifically if looking at the logarithmic plot. The data seems to describe a curve (which is good), we could try to fit a polynom of order 2. This means that we will use (day^2, day, const) as features. This *polynomial expansion* of the original feature `day` can be generated by the `PolynomialExpansion` feature transformer.\n",
    "\n",
    "This means that we will slightly extend our pipeline as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    # YOUR CODE HERE\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict\n",
    "\n",
    "Now we will again fit the pipeline to retrieve a model and immediately apply the model to all cases in order to get the data for another plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training_records)\n",
    "pred = model.transform(country_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "\n",
    "The next visualization looks better, especially the critical part of the graph is estimated much better. Note that we did not use data before day 28, since there was no real growth before that day.\n",
    "\n",
    "Note that our predicted values are above the measured values. This can mean multiple things:\n",
    "* *Pessimist*: Our model does not perform as good as desired\n",
    "* *Optimist*: Actions taken by politics change the real model parameters in a favorable way, such that the real number of infections do not grow any more as predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.yscale('log')\n",
    "plt.plot(df[\"day\"], df[\"Confirmed\"], color='blue', lw=2)\n",
    "plt.plot(df[\"day\"], df[\"Predict\"], color='red', lw=2)\n",
    "plt.vlines(cut_day, ymin=0, ymax=400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the image looks quite promising on the logarithmic scale, let us have a look at the linear scale. We will notice that we overpredict the number of cases by a factor of two and our prediction will look even worse for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(df[\"day\"], df[\"Confirmed\"], color='blue', lw=2)\n",
    "plt.plot(df[\"day\"], df[\"Predict\"], color='red', lw=2)\n",
    "plt.vlines(cut_day, ymin=0, ymax=300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance\n",
    "\n",
    "Again we will use the `RegressionEvaluator` as before to quantify the prediction error. The error should be much lower now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    predictionCol='Predict', \n",
    "    labelCol='Confirmed', \n",
    "    metricName='rmse'\n",
    ")\n",
    "\n",
    "pred = model.transform(validation_records)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Change training/validation split\n",
    "\n",
    "If we change the split of training to validation, things look much better. Of course this might be already expected, since we predict less data, but even the non-logarithmic plot looks really good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 80% for training (it was 70% before)\n",
    "cut_day = all_days[int(0.8*num_days)]\n",
    "print(\"cut_day = \" + str(cut_day))\n",
    "\n",
    "training_records_80 = country_cases.where((f.col(\"day\") <= cut_day) & (f.col(\"day\") >= first_day)).cache()\n",
    "validation_records_80 = country_cases.where(f.col(\"day\") > cut_day).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training_records_80)\n",
    "pred = model.transform(country_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred.toPandas()\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(df[\"day\"], df[\"Confirmed\"], color='blue', lw=2)\n",
    "plt.plot(df[\"day\"], df[\"Predict\"], color='red', lw=2)\n",
    "plt.vlines(cut_day, ymin=0, ymax=120000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result should let you look optimistic into the future, as it may indicate that the underlying process really has changed between day 50 and 60 and that the infection really slows down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure performance\n",
    "\n",
    "Again we will use the `RegressionEvaluator` as before to quantify the prediction error. The error should be much lower now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    predictionCol='Predict', \n",
    "    labelCol='Confirmed', \n",
    "    metricName='rmse'\n",
    ")\n",
    "\n",
    "pred = model.transform(validation_records)\n",
    "evaluator.evaluate(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final Note\n",
    "\n",
    "As already mentioned in the beginning, the whole approach is somewhat questionable. We are throwing a very generic machinery at a very specific problem which has a very specific structure. Therefore other approaches involving more meaningful models like https://de.wikipedia.org/wiki/SIR-Modell could give better prediction results. But those models require a completely different numerical approach for fitting the model to the data. We used the tool at hand (in this case PySpark) to generate a model, which does only make very mild (and possibly wrong) assumptions about the development process of the desease. Nevertheless such approaches might also give good results, since on the other hand specific mathematical models also rely on very specific assumptions and simplifications, which may also not be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
