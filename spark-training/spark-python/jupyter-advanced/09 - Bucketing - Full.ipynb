{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing\n",
    "\n",
    "We already saw that using a prepartitioned DataFrame for joins and grouped aggregations can accelerate execution time, but so far the prepartitioning had to be performed every time data is loaded from disk. It would be really nice, if there was some way to store prepartitioned data, such that Spark understands which columns were used to create the partitions.\n",
    "\n",
    "This is where bucketing comes into play, which is a special way to store data, such that Spark actually understands the partitioning schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Spark config\n",
    "Again, we need to disable automatic broadcast joins and make sure that bucketing is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.sources.bucketing.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "First we load the weather data, which consists of the measurement data and some station metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Measurements\n",
    "\n",
    "Measurements are stored in multiple directories (one per year). But we will limit ourselves to a single year in the analysis to improve readability of execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Read in all years, store them in an Python array\n",
    "raw_weather_per_year = [\n",
    "    spark.read.text(storageLocation + \"/\" + str(i)).withColumn(\"year\", lit(i))\n",
    "    for i in range(2003, 2015)\n",
    "]\n",
    "\n",
    "# Union all years together\n",
    "raw_weather = reduce(lambda l, r: l.union(r), raw_weather_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single year to keep execution plans small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", lit(2003))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Measurements\n",
    "\n",
    "Measurements were stored in a proprietary text based format, with some values at fixed positions. We need to extract these values with a simple `SELECT` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = raw_weather.select(\n",
    "    col(\"year\"),\n",
    "    substring(col(\"value\"), 5, 6).alias(\"usaf\"),\n",
    "    substring(col(\"value\"), 11, 5).alias(\"wban\"),\n",
    "    substring(col(\"value\"), 16, 8).alias(\"date\"),\n",
    "    substring(col(\"value\"), 24, 4).alias(\"time\"),\n",
    "    substring(col(\"value\"), 42, 5).alias(\"report_type\"),\n",
    "    substring(col(\"value\"), 61, 3).alias(\"wind_direction\"),\n",
    "    substring(col(\"value\"), 64, 1).alias(\"wind_direction_qual\"),\n",
    "    substring(col(\"value\"), 65, 1).alias(\"wind_observation\"),\n",
    "    (substring(col(\"value\"), 66, 4).cast(\"float\") / lit(10.0)).alias(\"wind_speed\"),\n",
    "    substring(col(\"value\"), 70, 1).alias(\"wind_speed_qual\"),\n",
    "    (substring(col(\"value\"), 88, 5).cast(\"float\") / lit(10.0)).alias(\"air_temperature\"),\n",
    "    substring(col(\"value\"), 93, 1).alias(\"air_temperature_qual\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Station Metadata\n",
    "\n",
    "We also need to load the weather station meta data containing information about the geo location, country etc of individual weather stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read.option(\"header\", True).csv(storageLocation + \"/isd-history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Bucketing Data\n",
    "\n",
    "Now we want to create a so called *bucketed table* of the weather measurements data. Bucketing is only possible within Hive, because additional meta data about the bucketing is required. That meta data is not stored on HDFS but persisted in the Hive metastore instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create Hive Table\n",
    "\n",
    "A bucketed Hive table can easily be created from within Spark by using the `bucketBy` and optionally `sortBy` method of the `DataFrameWriter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.write.bucketBy(200, \"usaf\", \"wban\").sortBy(\"usaf\", \"wban\").mode(\n",
    "    \"overwrite\"\n",
    ").saveAsTable(\"weather_buckets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspect Table\n",
    "\n",
    "We can inspect the Hive table, which unverils that both bucketing columns and sorting columns are present in the Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                year|                 int|   null|\n",
      "|                usaf|              string|   null|\n",
      "|                wban|              string|   null|\n",
      "|                date|              string|   null|\n",
      "|                time|              string|   null|\n",
      "|         report_type|              string|   null|\n",
      "|      wind_direction|              string|   null|\n",
      "| wind_direction_qual|              string|   null|\n",
      "|    wind_observation|              string|   null|\n",
      "|          wind_speed|              double|   null|\n",
      "|     wind_speed_qual|              string|   null|\n",
      "|     air_temperature|              double|   null|\n",
      "|air_temperature_qual|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|     weather_buckets|       |\n",
      "|               Owner|              hadoop|       |\n",
      "|        Created Time|Sat Oct 20 07:24:...|       |\n",
      "|         Last Access|Thu Jan 01 00:00:...|       |\n",
      "|          Created By|         Spark 2.3.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|         Num Buckets|                 200|       |\n",
      "|      Bucket Columns|    [`usaf`, `wban`]|       |\n",
      "|        Sort Columns|    [`usaf`, `wban`]|       |\n",
      "|    Table Properties|[transient_lastDd...|       |\n",
      "|          Statistics|       6353896 bytes|       |\n",
      "|            Location|hdfs://ip-10-200-...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "|         InputFormat|org.apache.hadoop...|       |\n",
      "|        OutputFormat|org.apache.hadoop...|       |\n",
      "|  Storage Properties|[serialization.fo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED weather_buckets\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CREATE TABLE` statement\n",
    "\n",
    "We could also have used Hive SQL to create the table. Let's retrieve the statement via SQL `SHOW CREATE TABLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE `weather_buckets` (`year` INT, `usaf` STRING, `wban` STRING, `date` STRING, `time` STRING, `report_type` STRING, `wind_direction` STRING, `wind_direction_qual` STRING, `wind_observation` STRING, `wind_speed` DOUBLE, `wind_speed_qual` STRING, `air_temperature` DOUBLE, `air_temperature_qual` STRING)\n",
      "USING parquet\n",
      "OPTIONS (\n",
      "  `serialization.format` '1'\n",
      ")\n",
      "CLUSTERED BY (usaf, wban)\n",
      "SORTED BY (usaf, wban)\n",
      "INTO 200 BUCKETS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stmts = spark.sql(\"SHOW CREATE TABLE weather_buckets\").collect()\n",
    "print(stmts[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Files\n",
    "Of couse there also need to be some files in HDFS now. These are stored in the directory `/user/hive/warehouse/weather_buckets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 107 items\r\n",
      "-rw-r--r--   1 hadoop hadoop          0 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/_SUCCESS\r\n",
      "-rw-r--r--   1 hadoop hadoop      89610 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00013.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      70655 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00016.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      84538 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00034.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      73316 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00035.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      80773 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00046.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      84986 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00048.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      72200 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00049.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      75935 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00058.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     138083 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00059.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      62817 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00084.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      76141 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00098.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      48870 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00099.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      79702 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00104.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78265 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00111.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      90587 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00113.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      97380 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00137.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      68907 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00141.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      80649 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00145.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      52018 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00151.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      79631 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00152.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     100585 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00163.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     163375 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00165.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78075 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00173.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78521 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00000-569127b5-0a94-4e65-ba66-dbea0babe322_00181.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      79693 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00011.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      73063 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00014.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     115209 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00025.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      62281 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00031.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      57775 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00039.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      59897 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00049.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      75265 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00050.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      64886 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00068.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      73180 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00078.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      61306 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00102.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      74272 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00104.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      54111 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00114.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      66911 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00119.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      79337 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00124.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      73118 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00127.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      72475 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00129.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78429 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00154.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      63448 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00158.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      56188 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00164.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      69343 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00174.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      69376 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00182.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      70658 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00187.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      56766 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00191.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78657 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00192.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      50076 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00195.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      78921 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00001-569127b5-0a94-4e65-ba66-dbea0babe322_00198.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      63340 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00005.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      48825 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00011.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      61512 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00017.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      67114 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00025.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      60629 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00033.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     123655 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00036.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      67351 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00040.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      55996 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00041.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      59784 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00043.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      56794 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00059.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      40241 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00066.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      36296 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00068.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      60617 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00108.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      62836 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00122.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      56994 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00129.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      75858 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00130.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      86810 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00132.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      57741 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00133.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      37198 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00143.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     135687 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00156.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     142939 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00157.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      61759 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00166.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      56997 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00178.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      60186 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00002-569127b5-0a94-4e65-ba66-dbea0babe322_00199.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      24241 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00003.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      32695 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00006.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      67545 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00026.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      87515 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00028.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      18292 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00031.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      16246 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00032.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      12767 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00033.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      37920 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00038.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      60268 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00053.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      29008 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00071.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      34895 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00092.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      21333 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00096.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      31191 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00100.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      31085 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00112.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       8183 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00114.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      22701 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00118.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop     107950 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00122.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      26162 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00137.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      12477 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00150.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      17235 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00154.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      36580 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00163.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      18942 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00171.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       8239 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00172.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      37877 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00178.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      30513 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00179.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      15683 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00186.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop      33030 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00003-569127b5-0a94-4e65-ba66-dbea0babe322_00189.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       3202 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00004-569127b5-0a94-4e65-ba66-dbea0babe322_00058.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       3393 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00004-569127b5-0a94-4e65-ba66-dbea0babe322_00081.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       3359 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00004-569127b5-0a94-4e65-ba66-dbea0babe322_00088.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       2790 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00004-569127b5-0a94-4e65-ba66-dbea0babe322_00130.c000.snappy.parquet\r\n",
      "-rw-r--r--   1 hadoop hadoop       3160 2018-10-20 07:24 /user/hive/warehouse/weather_buckets/part-00004-569127b5-0a94-4e65-ba66-dbea0babe322_00134.c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/weather_buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Bucketing & Joins\n",
    "\n",
    "Now we can perform the same join again, but this time against the bucketed version of the weather table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Normal Join\n",
    "\n",
    "To see the effect of bucketing, we first perform a traditional join to see the execution plan as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [usaf#87, wban#88], [usaf#122, wban#123], Inner\n",
      ":- *(2) Sort [usaf#87 ASC NULLS FIRST, wban#88 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      ":     +- *(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      ":        +- *(1) Filter (isnotnull(substring(value#82, 11, 5)) && isnotnull(substring(value#82, 5, 6)))\n",
      ":           +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n",
      "+- *(4) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(3) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(3) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(3) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "result = weather.join(\n",
    "    stations,\n",
    "    (weather[\"usaf\"] == stations[\"usaf\"]) & (weather[\"wban\"] == stations[\"wban\"]),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Bucketed Join\n",
    "\n",
    "Now we want to replace the original `weather` DataFrame by a bucketed version. This means that first we have to create a bucketed version in HDFS. This is only possible by creating a Hive table, since this is the only way to persist the bucketing information as table properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [usaf#248, wban#249], [usaf#122, wban#123], Inner\n",
      ":- *(1) Sort [usaf#248 ASC NULLS FIRST, wban#249 ASC NULLS FIRST], false, 0\n",
      ":  +- *(1) Project [year#247, usaf#248, wban#249, date#250, time#251, report_type#252, wind_direction#253, wind_direction_qual#254, wind_observation#255, wind_speed#256, wind_speed_qual#257, air_temperature#258, air_temperature_qual#259]\n",
      ":     +- *(1) Filter (isnotnull(wban#249) && isnotnull(usaf#248))\n",
      ":        +- *(1) FileScan parquet default.weather_buckets[year#247,usaf#248,wban#249,date#250,time#251,report_type#252,wind_direction#253,wind_direction_qual#254,wind_observation#255,wind_speed#256,wind_speed_qual#257,air_temperature#258,air_temperature_qual#259] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-10-200-101-213.eu-central-1.compute.internal:8020/user/hive/warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(wban), IsNotNull(usaf)], ReadSchema: struct<year:int,usaf:string,wban:string,date:string,time:string,report_type:string,wind_direction...\n",
      "+- *(3) Sort [usaf#122 ASC NULLS FIRST, wban#123 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(usaf#122, wban#123, 200)\n",
      "      +- *(2) Project [USAF#122, WBAN#123, STATION NAME#124, CTRY#125, STATE#126, ICAO#127, LAT#128, LON#129, ELEV(M)#130, BEGIN#131, END#132]\n",
      "         +- *(2) Filter (isnotnull(usaf#122) && isnotnull(wban#123))\n",
      "            +- *(2) FileScan csv [USAF#122,WBAN#123,STATION NAME#124,CTRY#125,STATE#126,ICAO#127,LAT#128,LON#129,ELEV(M)#130,BEGIN#131,END#132] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/isd-history], PartitionFilters: [], PushedFilters: [IsNotNull(USAF), IsNotNull(WBAN)], ReadSchema: struct<USAF:string,WBAN:string,STATION NAME:string,CTRY:string,STATE:string,ICAO:string,LAT:strin...\n"
     ]
    }
   ],
   "source": [
    "weather_hive = spark.read.table(\"weather_buckets\")\n",
    "result = weather_hive.join(\n",
    "    stations,\n",
    "    (weather_hive[\"usaf\"] == stations[\"usaf\"])\n",
    "    & (weather_hive[\"wban\"] == stations[\"wban\"]),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "The execution plan now looks differently than before.\n",
    "* The station meta data is still shuffled (we didn't bucketize it)\n",
    "* The weather data does not require a shuffle any more, the join can be executed almost directly (A sort will still be performed, maybe a bug?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing Strategie\n",
    "\n",
    "The following attributes have to match\n",
    "* bucketing columns\n",
    "* number of buckets = number of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Bucketing & Aggregation\n",
    "\n",
    "Similar to `JOIN` operations, grouped aggregations (`GROUP BY`) also require a shuffle operation. Again this can be avoided if a Hive table is used that is already bucketed according to the grouping columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Normal Aggregation\n",
    "\n",
    "First let us analyze the execution plan of a normal grouped aggregation operation without a bucketed table. This will result in an execution plan containing a shuffle operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[usaf#87, wban#88], functions=[min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "+- Exchange hashpartitioning(usaf#87, wban#88, 200)\n",
      "   +- *(1) HashAggregate(keys=[usaf#87, wban#88], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END), partial_max(CASE WHEN (cast(air_temperature_qual#98 as int) = 1) THEN air_temperature#97 END)])\n",
      "      +- *(1) Project [substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "         +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "result = weather.groupBy(weather[\"usaf\"], weather[\"wban\"]).agg(\n",
    "    min(when(weather.air_temperature_qual == lit(1), weather.air_temperature)).alias(\n",
    "        'min_temp'\n",
    "    ),\n",
    "    max(when(weather.air_temperature_qual == lit(1), weather.air_temperature)).alias(\n",
    "        'max_temp'\n",
    "    ),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "As expected the execution plan has three steps related to the grouped aggregation:\n",
    "1. Partial aggregate (`HashAggregate`)\n",
    "2. Shuffle operation (`Exchange hashpartitioning`)\n",
    "3. Final aggregate (`HashAggregate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Bucketed Aggregation\n",
    "\n",
    "Now let's perform the same operation, but this time using the bucketed Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) HashAggregate(keys=[usaf#248, wban#249], functions=[min(CASE WHEN (cast(air_temperature_qual#259 as int) = 1) THEN air_temperature#258 END), max(CASE WHEN (cast(air_temperature_qual#259 as int) = 1) THEN air_temperature#258 END)])\n",
      "+- *(1) HashAggregate(keys=[usaf#248, wban#249], functions=[partial_min(CASE WHEN (cast(air_temperature_qual#259 as int) = 1) THEN air_temperature#258 END), partial_max(CASE WHEN (cast(air_temperature_qual#259 as int) = 1) THEN air_temperature#258 END)])\n",
      "   +- *(1) FileScan parquet default.weather_buckets[usaf#248,wban#249,air_temperature#258,air_temperature_qual#259] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-10-200-101-213.eu-central-1.compute.internal:8020/user/hive/warehouse..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<usaf:string,wban:string,air_temperature:double,air_temperature_qual:string>\n"
     ]
    }
   ],
   "source": [
    "result = weather_hive.groupBy(weather_hive[\"usaf\"], weather_hive[\"wban\"]).agg(\n",
    "    min(\n",
    "        when(weather_hive.air_temperature_qual == lit(1), weather_hive.air_temperature)\n",
    "    ).alias('min_temp'),\n",
    "    max(\n",
    "        when(weather_hive.air_temperature_qual == lit(1), weather_hive.air_temperature)\n",
    "    ).alias('max_temp'),\n",
    ")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "As we hoped for, Spark will not perform a shuffle operation any more, since the data is already partitioned as needed. The execution plan now only contains two steps for implementing the grouped aggregation\n",
    "1. Partial aggregate (`HashAggregate`)\n",
    "2. Final aggregate (`HashAggregate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bucketing & Filtering\n",
    "\n",
    "Unfortunately Spark does not use bucketing information for filtering yet. Let's prove that by a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Filter without bucketing\n",
    "\n",
    "Let read in the raw data and add a filter operation that refers to the bucketing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [2003 AS year#84, substring(value#82, 5, 6) AS usaf#87, substring(value#82, 11, 5) AS wban#88, substring(value#82, 16, 8) AS date#89, substring(value#82, 24, 4) AS time#90, substring(value#82, 42, 5) AS report_type#91, substring(value#82, 61, 3) AS wind_direction#92, substring(value#82, 64, 1) AS wind_direction_qual#93, substring(value#82, 65, 1) AS wind_observation#94, (cast(cast(substring(value#82, 66, 4) as float) as double) / 10.0) AS wind_speed#95, substring(value#82, 70, 1) AS wind_speed_qual#96, (cast(cast(substring(value#82, 88, 5) as float) as double) / 10.0) AS air_temperature#97, substring(value#82, 93, 1) AS air_temperature_qual#98]\n",
      "+- *(1) Filter ((isnotnull(value#82) && (substring(value#82, 5, 6) = 123)) && (substring(value#82, 11, 5) = 456))\n",
      "   +- *(1) FileScan text [value#82] Batched: false, Format: Text, Location: InMemoryFileIndex[s3://dimajix-training/data/weather/2003], PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "result = weather.where(\"usaf = '123' AND wban='456'\")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Filter with bucketing\n",
    "\n",
    "Now let's try the same example, but this time we use the bucketed Hive table instead of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [year#247, usaf#248, wban#249, date#250, time#251, report_type#252, wind_direction#253, wind_direction_qual#254, wind_observation#255, wind_speed#256, wind_speed_qual#257, air_temperature#258, air_temperature_qual#259]\n",
      "+- *(1) Filter (((isnotnull(usaf#248) && isnotnull(wban#249)) && (usaf#248 = 123)) && (wban#249 = 456))\n",
      "   +- *(1) FileScan parquet default.weather_buckets[year#247,usaf#248,wban#249,date#250,time#251,report_type#252,wind_direction#253,wind_direction_qual#254,wind_observation#255,wind_speed#256,wind_speed_qual#257,air_temperature#258,air_temperature_qual#259] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-10-200-101-213.eu-central-1.compute.internal:8020/user/hive/warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(usaf), IsNotNull(wban), EqualTo(usaf,123), EqualTo(wban,456)], ReadSchema: struct<year:int,usaf:string,wban:string,date:string,time:string,report_type:string,wind_direction...\n"
     ]
    }
   ],
   "source": [
    "result = weather_hive.where(\"usaf = '123' AND wban='456'\")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "The execution plan contains *`PushedFilters`*, but the Spark web ui will reveil, that these filters are only pushed down to the parquet reader and Spark still reads all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 2.3 (Python 3)",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
