{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ\n",
    "\n",
    "Then, please feel free to ask:\n",
    "\n",
    "https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n",
    "\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">' + string + \"</span>\"))\n",
    "\n",
    "\n",
    "if \"sc\" in locals() or \"sc\" in globals():\n",
    "    printmd(\n",
    "        \"<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd(\n",
    "        \"<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.1\n",
    "Welcome to Exercise 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n",
    "!mv washing.parquet?raw=true washing.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"washing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's check how may rows we have got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we register the data frame in the ApacheSparkSQL catalog so that we can query it using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"washing\")\n",
    "spark.sql(\"SELECT * FROM washing\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's visualize voltage using a box plot to get an idea on the value distribution of this parameter. First, we have to create a python list. Make sure you use the sample function in order to not blast your spark driver or plotting library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"select voltage from washing where voltage is not null\")\n",
    "result_array = result.rdd.map(lambda row: row.voltage).sample(False, 0.1).collect()\n",
    "\n",
    "# just print the 1st 15 elements\n",
    "result_array[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to activate the notebook to show the plots directly under the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to plot the python list by importing the matplotlib library, calling \"boxplot\" and \"show\". Note that you can see mean (red line) around 230. Then you see that 50% of all values are between 225 and 235 (blue box). Per default values up to 250 are not seen as outliers (little, black horizontal line). And the \"plus\" symbols on top are definitely outliers. Congratulations, you've written your first anomaly detection algorithm. Unfurtunately you still need a brain attached to it, so we'll cover on how we write one without a brain needed in the next course. And in the course after that we'll even tell you how to implement a artificial brain to further improve it, so stay tuned :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.boxplot(result_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with time series data we want to make use of the time dimension as well. The least complex plots are run charts where the time domain (dimension) is represented at the horizontal x-axis and the y-axis shows the actual sensor value. Let's do this for voltage as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\n",
    "    \"select voltage,ts from washing where voltage is not null order by ts asc\"\n",
    ")\n",
    "result_rdd = result.rdd.sample(False, 0.1).map(lambda row: (row.ts, row.voltage))\n",
    "result_array_ts = result_rdd.map(lambda ts_voltage: ts_voltage[0]).collect()\n",
    "result_array_voltage = result_rdd.map(lambda ts_voltage: ts_voltage[1]).collect()\n",
    "print(result_array_ts[:15])\n",
    "print(result_array_voltage[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result_array_ts, result_array_voltage)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"voltage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time we want to only plot data worth of one hour. Therefore we first have to find out in which date range we have data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select min(ts),max(ts) from washing\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets repeat the previous steps but only show data for hour. We've found out the low and high boundary of data available and we know that \"ts\" stand for \"timestamp\". Timestamp are the number of millisecons passed since the 1st of Jan. 1970. You can also use an online tool like http://www.epochconverter.com/ to convert these. But for now just an interval of 60 minutes (1000*60*60)=3600000 within the range above (note that we have removed the sample function because the data set is already reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "select voltage,ts from washing \n",
    "    where voltage is not null and \n",
    "    ts > 1547808720911 and\n",
    "    ts <= 1547810064867+3600000\n",
    "    order by ts asc\n",
    "\"\"\"\n",
    ")\n",
    "result_rdd = result.rdd.map(lambda row: (row.ts, row.voltage))\n",
    "result_array_ts = result_rdd.map(lambda ts_voltage: ts_voltage[0]).collect()\n",
    "result_array_voltage = result_rdd.map(lambda ts_voltage: ts_voltage[1]).collect()\n",
    "plt.plot(result_array_ts, result_array_voltage)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"voltage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are not only able to spot the outliers but also see a time pattern of these outliers occuring which can be used for further downstream analysis. Again your brain was already capable of spotting the pattern. In the next two coursera courses we will teach a machine to spot those patterns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've plotted a maximum of two dimensions at a time, let's go for three in a so-called 3D scatter plot. Again we have to create python lists (with applied sampling if necessary) from three properties of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = spark.sql(\n",
    "    \"\"\"\n",
    "select hardness,temperature,flowrate from washing\n",
    "    where hardness is not null and \n",
    "    temperature is not null and \n",
    "    flowrate is not null\n",
    "\"\"\"\n",
    ")\n",
    "result_rdd = result_df.rdd.sample(False, 0.1).map(\n",
    "    lambda row: (row.hardness, row.temperature, row.flowrate)\n",
    ")\n",
    "result_array_hardness = result_rdd.map(\n",
    "    lambda hardness_temperature_flowrate: hardness_temperature_flowrate[0]\n",
    ").collect()\n",
    "result_array_temperature = result_rdd.map(\n",
    "    lambda hardness_temperature_flowrate: hardness_temperature_flowrate[1]\n",
    ").collect()\n",
    "result_array_flowrate = result_rdd.map(\n",
    "    lambda hardness_temperature_flowrate: hardness_temperature_flowrate[2]\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done it is very simple to import the necessary library and create a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "ax.scatter(\n",
    "    result_array_hardness,\n",
    "    result_array_temperature,\n",
    "    result_array_flowrate,\n",
    "    c=\"r\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"hardness\")\n",
    "ax.set_ylabel(\"temperature\")\n",
    "ax.set_zlabel(\"flowrate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the individual points resemble in some sort of plane. But this is not a surprise. Actually we can draw the following conclusions from the plot:\n",
    "\n",
    "- most of the data points are lying around hardness 60-80, temperature 80-100 and flowrate 80-100\n",
    "- there are some outliers, especially when it comes to the range of hardness 100-200\n",
    "- the data follows some narrow boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets double-check what's going on with hardness since it seems that it really sticks around 60-80 and very seldom creates values above that. We can use a histogram for that which bins together certain value ranges and counts the frequency of occurences of values within this range. Those frequencies are ordered and shown as a bar diagram, so let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result_array_hardness)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our assumpion was correct, nearly all values are around 60-80 with very less values about that threshold.\n",
    "This concludes Exercice 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
